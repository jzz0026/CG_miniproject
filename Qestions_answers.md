## 1.What features of the data are most important for QC/QA?
a.	Missing data are important to be considered. In this case, drop the columns full of nan for both mRNA expression and mutation sequencing data as well as individuals with labels containing missing data (e.g. Neoplasm Histologic Grade and Patient's Vital Status).

b. Since mRNA expression, mutation sequencing and clinical data would be combined together for modeling, it is important to make sure sample IDs were consistent between each other. Here I noticed only 432 samples existed in all three types of data, so in this study only intersection part among three were selected for further modeling in order to combine all features together.

c. Some outliers could be potential errors during sequencing or sampling. The high variability in clinical population could also cause outliers and affect the performance of models. Here Isolation Forest was applied to drop outliers since it was based on random forest and can handle large, high-dimensional datasets. PCA was then used for visualization of this filtering step. About 10.11% observations were filtered and PCA showed a more acceptable variance than before filtering.

## 2. Generally speaking, what are potential sources of ambiguity arising from your approach?
a. In unsupervised clustering, we assumed each of four features was equally to indicate risk due to limited domain knowledge. However, the four features didn’t correlated closely. Many individuals could have a high cancer grade but low stage and live fairly long after diagnosis, vice versa. They could contribute to risk differently, so it might be worth to changing weight to achieve a better balance of four features for unsupervised clustering. This step is fairly critical since it generate labels for downstream modeling and might a major source of ambiguity.

b. Clinical and sequencing data of human population could be distinct from individual to individual. The sample size we had right now could still be small, could lead to the problem of high variance. Despite the step of feature selection was applied, it could still be lack of enough information from individuals to indicate disease-associated risk, causing the ambiguity in our approach.


c. It could be from lack of environmental data. As we know, phenotype = clinical data + transcriptome + genotype + environmental data. The environmental data could include information of individuals’ diets, life styles, habits and so on, which would have potential affect on causation of cancers.  This could be a big source of ambiguity in our approach. 


## 3. What other data might we collect to enhance risk quantification? What quantitative proof do you have?
a. If we do whole genome sequencing, whole transcriptome and collected more samples of patients, I think it will be greatly helpful to enhance risk quantification. The heatmap of mRNA expression didn’t show any obvious pattern among both features and observations. It confirmed that observations had a high variance (over-fitting) and we need a larger sample size. Also, lack of pattern in features could indicate some strong predictors might not be covered in this study and we probably need to include more genes for sequencing.

b. In unsupervised clustering, we have imbalanced classes for four features, especially cancer grade (G1: 14, G2: 227, G3: 205, G4: 76, GX: 5) and stage (T1: 270, T2: 68, T3, 178, T4: 11). For example, most of patients were diagnosed at G2 and G3. Assume we had a native classifier that just randomly predict G2 and G3, it will still achieve an about 80% accuracy. This is pretty common to observed in cancer detection, since many people at late grade were already dead. So increasing samples size will definitely increase statistical power and freedom, leading to achieving a better performance of model. For the clustering step, this imbalance could also cause problem to label risk, since we didn’t have enough data in these categories. So it will be good that we can collect more individuals belonging to small classes (e.g. GX and T4).

c. Another quantitative approach is to use rarefaction curve that plotted sample size against train and test accuracies/errors.  If train and test curves were very different and not converged together, adding more samples will help to reduce variance. If train and test curves were converged, adding samples will not be help to improve this model but it could still be helpful if we switched to a more complex model.


## 4. Describe your approach to filing IP claims around your unique classification of risk?
The approach to build models here included cleaning, filling missing data, feature selection, apply multiple machine learning algorithms and evaluate their performance. The features full of missing data were firstly removed and then all categories were converted to numerical. Since the data had a high dimensionality, most of features with multiple categories were converted to integer rather than binary in order to avoid more dimensions.  Especially mutation data, they were changed to penalty scores based on the BLOSUM62 matrix, so that the data was less sparse and dimensional. Also, the RNA-Seq data have some outlier that could be derived from sequencing/sampling errors or overexpression of some genes. Isolation Forest was applied to drop outliers since it was based on random forest and can handle large, high-dimensional datasets. About 10.11% observations were filtered and PCA showed a more acceptable variance than before filtering. Filtered clinic, transcription and mutation data were combined together in order for modeling, leading to totally 432 observations. 

As we known, a clinician uses a combination of cancer stage, grade, overall survival in months following diagnosis, and vital status (alive/dead) to establish risk. Since this risk was not provided, we should also look into these features and try to build risk as the label for modeling. Intuitively, both doctor and patients care about length of the remaining life most, so overall survival in months should be the most indicative to build risk desipte 68% of individuals are still alive and could cause bias if only using overall survival as risk. Cancer stage and grade are also very meaningful to represent danger and risk, but could be incomplete and cause misleading if used alone. Many individuals could have a high cancer grade but low stage and live fairly long after diagnosis, vice versa. Here we assume each of four features was equally to indicate risk due to limited domain knowledge. After comparison of EM and Kmeans algorithms, Kmeans with 2 components was finally applied for clustering, leading to two clusters of high and low risk (187 and 340 observations, respectively), since it tended to give a good balance on four features. The high risk tended to have higher danger level of stage and grade, live shorter after diagnosis and contain more individuals that was already dead than the low risk.



## 5.	How would you communicate your findings to a clinician?
Clinic, transcription and mutation data were combined together in order to utilize more information for modeling. The samples were directly derived from urine and noninvasive, followed by measurement and sequencing. So patients will have no pain and low cost for cancer detection. The modeling process included cleaning, filling missing data, feature selection, apply multiple machine learning algorithms and evaluate their performance. The step of feature selection is important since high dimensionalities will cause over-fitting for many algorithms.

The best model was built based on the top correlation selection (select top 20 feature with strongest correlation with risk) and Random Forest Classifier (ensemble results from 3000 estimators predicted based on bootstrap sampling of observations and subsampling of features) with an accuracy of 0.78 and roc-auc of 0.80. The obtained accuracy is 16% better than baseline (choose the most frequent class), demonstrating a good performance on prediction of Disease-associated risk.

The prediction result could be an additional support for doctors’ diagnose. In the future, we have confidence to achieve a better performance if we can collect more patience data. The more data (both examples and useful features) will help to find similarities and differences within and between classes.  It will be great if you can provide us more data to improve the model.  We are looking forwards to more opportunities to collaborate with you!

 


